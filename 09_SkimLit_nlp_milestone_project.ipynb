{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfPJY0DOUZFSSgtc5/xAab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PengJuan-AI/TensorFlow_Study/blob/main/09_SkimLit_nlp_milestone_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone Project 2: SkimLit 📝\n",
        "** The purpose of this notebook is to build an NLP model to make reading medical abstracts easier.**\n",
        "\n",
        "### Original Paper\n",
        "The paper we're replicating is [a Dataset for Sequential Sentence Classification in Medical Abstracts](https://arxiv.org/abs/1710.06071).\n",
        "\n",
        "The model architecture that they use to achieve the best results is here: https://arxiv.org/pdf/1612.05251.pdf\n",
        "\n",
        "When it was released, the paper presented a new dataset called PubMed 200k RCT which consists of ~200,000 labelled Randomized Controlled Trial (RCT) abstracts.\n",
        "\n",
        "The goal of the dataset was to explore the ability for NLP models to classify sentences which appear in sequential order.\n",
        "\n",
        "In other words, given the abstract of a RCT, what role does each sentence serve in the abstract?\n",
        "\n",
        "## What is the task\n",
        "What the Skimlit model doing is to transfer a hard-to-read abstract into a easier-to-read one. (many-to-one problem)\n",
        "* Input -  a sentence in one abstract\n",
        "* Output - a label\n",
        "\n",
        "What is going to include in this project:\n",
        "* Downloading a text dataset.\n",
        "* Writing a preprocessing function for our text data.\n",
        "* Setting up multiple modelling experiments _with different levels of embeddings_.\n",
        "* Building _multimodal model_ to take in different sources of data.\n",
        "* Finding the most wrong prediction examples.\n",
        "\n",
        "## Additional\n",
        "\n",
        "Some motivation!\n",
        "\n",
        "> Machine Learning Engineer\n",
        "> 1. Download a paper.\n",
        "> 2. Implement it.\n",
        "> 3. Keep doing this until you have skills.\n",
        "> 4. Machine learning engineering also involves building infrastructure around your model/data preprocessings steps."
      ],
      "metadata": {
        "id": "YXv9r1wxUEy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get helper function"
      ],
      "metadata": {
        "id": "iNdWd5LQzvnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get helper functions we write before\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op6zVF1tzyeN",
        "outputId": "6f95a693-6df2-4d59-fc97-242822329b23"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-18 00:13:53--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-18 00:13:53 (98.7 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import calculate_results helper function\n",
        "from helper_functions import calculate_results"
      ],
      "metadata": {
        "id": "rDnw-qMMz6gf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data\n",
        "\n",
        "Since we'll be replicating the paper, let's download the dataset they used (PubMed 200k RCT)\n",
        "\n",
        "Find the data of a paper on Github: https://github.com/Franck-Dernoncourt/pubmed-rct"
      ],
      "metadata": {
        "id": "hAJEe0MaS_qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "!ls pubmed-rct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZleoKVTctt-",
        "outputId": "7f96e701-c5bc-4484-e30c-ba46b89cac0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pubmed-rct'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 33 (delta 5), reused 5 (delta 5), pack-reused 25\u001b[K\n",
            "Receiving objects: 100% (33/33), 177.08 MiB | 24.34 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "Updating files: 100% (13/13), done.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PubMed 20k is a subset of PubMed 200k, we experiment on 20k dataset and then scale up to the whole dataset.\n",
        "\n",
        "The first step should be scan through the dataset (or subsets) to check the format of the dataset. Further we need to write a function to preprocess the dataset so that make it ready for our model."
      ],
      "metadata": {
        "id": "fZ0Ng0yYdB9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what files are in the PubMed_20k dataset\n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/ # this dataset replace number with @"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUZjTnynczb4",
        "outputId": "8a74c9e2-6f5e-4e61-9056-041788ed3f95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev.txt  test.txt  train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ],
      "metadata": {
        "id": "qwqOgp3YdhGX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0jkz350pwap",
        "outputId": "5076be79-539c-41aa-a429-37079d3ee1dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data\n",
        "\n",
        "**Become One with Data**\n",
        "\n",
        "> Visualize, visualize, visualize\n",
        "\n",
        "Read file with Python: https://realpython.com/read-write-files-python/"
      ],
      "metadata": {
        "id": "0syJtI-Bp73e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read all lines of target files\n",
        "def get_lines(filename):\n",
        "  \"\"\"\n",
        "  Reads filename and returns the lines pf text as a list\n",
        "\n",
        "  Args:\n",
        "    filename: a string containing the target filepath.\n",
        "\n",
        "  Return:\n",
        "    A list of strings with one string per line from the target file.\n",
        "  \"\"\"\n",
        "  with open(filename, 'r') as f:\n",
        "    return f.readlines()"
      ],
      "metadata": {
        "id": "TJrqqDpeqcIQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read in the training lines\n",
        "train_lines = get_lines(data_dir+\"train.txt\") # read the lines in the training file\n",
        "train_lines[:35]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG4Vf0I6rQQN",
        "outputId": "e82fbb19-cf1d-4e08-b1fe-e1523d1764d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['###24293578\\n',\n",
              " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
              " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
              " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
              " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
              " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
              " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
              " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
              " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
              " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
              " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
              " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
              " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
              " '\\n',\n",
              " '###24854809\\n',\n",
              " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
              " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
              " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
              " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
              " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n',\n",
              " 'METHODS\\tAttentional biases for high caloric foods were measured by eye tracking during a visual probe task with pictorial food and neutral stimuli .\\n',\n",
              " 'METHODS\\tSelf-reported emotional eating was assessed with the Dutch Eating Behavior Questionnaire ( DEBQ ) and ad libitum food intake was tested by a disguised food offer .\\n',\n",
              " 'RESULTS\\tHierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .\\n',\n",
              " 'RESULTS\\tYet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .\\n',\n",
              " 'CONCLUSIONS\\tThe current findings show that self-reported emotional eating ( based on the DEBQ ) might not validly predict who overeats when sad , at least not in a laboratory setting with healthy women .\\n',\n",
              " 'CONCLUSIONS\\tResults further suggest that attention maintenance on food relates to eating motivation when in a neutral affective state , and might therefore be a cognitive mechanism contributing to increased food intake in general , but maybe not during sad mood .\\n',\n",
              " '\\n',\n",
              " '###25165090\\n',\n",
              " 'BACKGROUND\\tAlthough working smoke alarms halve deaths in residential fires , many households do not keep alarms operational .\\n',\n",
              " 'BACKGROUND\\tWe tested whether theory-based education increases alarm operability .\\n',\n",
              " 'METHODS\\tRandomised multiarm trial , with a single arm randomly selected for use each day , in low-income neighbourhoods in Maryland , USA .\\n',\n",
              " \"METHODS\\tIntervention arms : ( @ ) Full Education combining a health belief module with a social-cognitive theory module that provided hands-on practice installing alarm batteries and using the alarm 's hush button ; ( @ ) Hands-on Practice social-cognitive module supplemented by typical fire department education ; ( @ ) Current Norm receiving typical fire department education only .\\n\",\n",
              " 'METHODS\\tFour hundred and thirty-six homes recruited through churches or by knocking on doors in @-@ .\\n',\n",
              " 'METHODS\\tFollow-up visits checked alarm operability in @ homes ( @ % ) @-@ @ years after installation .\\n',\n",
              " 'METHODS\\tnumber of homes with working alarms defined as alarms with working batteries or hard-wired and number of working alarms per home .\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we want our data to look?\n",
        "\n",
        "```python\n",
        "# A list of dictory data structure\n",
        "[{'line_number': 0,\n",
        "  'target': RESULTS, # or METHODS or OBJECTIVE or BACKGROUND or CONCLUSION\n",
        "  'text': ...,\n",
        "  'total_lines': ...,\n",
        "}]\n",
        "```"
      ],
      "metadata": {
        "id": "Ug6rLrDUsARI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_with_line_numbers(filename):\n",
        "  \"\"\"Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "  Takes in filename, reads its contents and sorts through each line,\n",
        "  extracting things like the target label, the text of the sentence,\n",
        "  how many sentences are in the current abstract and what sentence number\n",
        "  the target line is.\n",
        "\n",
        "  Args:\n",
        "      filename: a string of the target text file to read and extract line data\n",
        "      from.\n",
        "\n",
        "  Returns:\n",
        "      A list of dictionaries each containing a line from an abstract,\n",
        "      the lines label, the lines position in the abstract and the total number\n",
        "      of lines in the abstract where the line is from. For example:\n",
        "\n",
        "      [{\"target\": 'CONCLUSION',\n",
        "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
        "        \"line_number\": 8,\n",
        "        \"total_lines\": 8}]\n",
        "  \"\"\"\n",
        "  input_lines = get_lines(filename) # get all lines from filename\n",
        "  abstract_lines = \"\" # create an empty abstract\n",
        "  abstract_samples = [] # create an empty list of abstracts\n",
        "\n",
        "  # Loop through each line in target file\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\" # reset abstract string\n",
        "    elif line.isspace(): # check to see if line is a new line\n",
        "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "\n",
        "      # Iterate through each line in abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {} # create empty dict to store data from line\n",
        "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0] # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "\n",
        "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
        "      abstract_lines += line\n",
        "\n",
        "  return abstract_samples"
      ],
      "metadata": {
        "id": "-K019Qodrqli"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_data(filename):\n",
        "  \"\"\"\n",
        "  Returns a list of dictionaries of abstract line data.\n",
        "\n",
        "  Take in filename, reads its contents through each line,\n",
        "  extracting attributes - line_number, target, text and total_line.\n",
        "  \"\"\"\n",
        "  inputlines = get_lines(filename)\n",
        "  abstract = []\n",
        "  processed_data = []\n",
        "  for line in inputlines:\n",
        "    if line.startswith(\"###\"): # is the first line of an abstract (skip)\n",
        "      if len(abstract) == 0:\n",
        "        continue\n",
        "      else: # comes to another '###' means that it comes to a new abstract, so we need to preprocess the last abstract\n",
        "        total_line = len(abstract)\n",
        "        for n,sentence in enumerate(abstract):\n",
        "          sentence = sentence.strip(\"\\n\").split('\\t')\n",
        "          # print(f\"Line number is {n}\\n {sentence}\")\n",
        "          abstract_dict = {\"line_number\": n,\n",
        "                         \"target\": sentence[0],\n",
        "                         \"text\": sentence[1].lower(),\n",
        "                         \"total_line\": total_line}\n",
        "          processed_data.append(abstract_dict)\n",
        "        # reset the abstract contents\n",
        "        abstract = []\n",
        "    # elif line=='\\n': # skip\n",
        "    elif line.isspace():\n",
        "      continue\n",
        "    else:\n",
        "      abstract.append(line)\n",
        "\n",
        "  return processed_data"
      ],
      "metadata": {
        "id": "UIKLgqritfB7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_samples = preprocess_text_data(data_dir+'train.txt')\n",
        "val_samples = preprocess_text_data(data_dir+'dev.txt') # dev.txt is another name for validation data\n",
        "test_samples = preprocess_text_data(data_dir+'test.txt')\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2mN3GbFvyYh",
        "outputId": "1e609361-4cea-4a55-bbb6-e544734c60a1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 250 ms, sys: 86.9 ms, total: 337 ms\n",
            "Wall time: 342 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180028, 30198, 30117)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try functions in course\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir+'train.txt')\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir+'dev.txt')\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir+'test.txt')\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS6He4Dm6HlK",
        "outputId": "f198e0e9-8093-407c-e865-e3b1585bf9e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 296 ms, sys: 55 ms, total: 351 ms\n",
            "Wall time: 360 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9znYIHUpYRP",
        "outputId": "c76c0347-9909-4172-f20c-88b4be694d90"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'target': 'OBJECTIVE',\n",
              "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
              "  'line_number': 0,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
              "  'line_number': 1,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
              "  'line_number': 2,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
              "  'line_number': 3,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
              "  'line_number': 4,\n",
              "  'total_lines': 11},\n",
              " {'target': 'METHODS',\n",
              "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
              "  'line_number': 5,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
              "  'line_number': 6,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
              "  'line_number': 7,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
              "  'line_number': 8,\n",
              "  'total_lines': 11},\n",
              " {'target': 'RESULTS',\n",
              "  'text': 'these differences remained significant at @ weeks .',\n",
              "  'line_number': 9,\n",
              "  'total_lines': 11}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn it into DataFrame to further visualize it."
      ],
      "metadata": {
        "id": "41-6qiLTpmDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)"
      ],
      "metadata": {
        "id": "E6_bTm1Ppryv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "b0TVIyDsp7Wg",
        "outputId": "146fb446-5ffc-431c-cc19-4378f56859a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      target                                               text  line_number  \\\n",
              "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
              "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
              "2    METHODS  outcome measures included pain reduction and i...            2   \n",
              "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
              "4    METHODS  secondary outcome measures included the wester...            4   \n",
              "\n",
              "   total_lines  \n",
              "0           11  \n",
              "1           11  \n",
              "2           11  \n",
              "3           11  \n",
              "4           11  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a104bec7-1b86-4a0e-8221-509b97a4ed22\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>text</th>\n",
              "      <th>line_number</th>\n",
              "      <th>total_lines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OBJECTIVE</td>\n",
              "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>a total of @ patients with primary knee oa wer...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>outcome measures included pain reduction and i...</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>pain was assessed using the visual analog pain...</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>METHODS</td>\n",
              "      <td>secondary outcome measures included the wester...</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a104bec7-1b86-4a0e-8221-509b97a4ed22')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a104bec7-1b86-4a0e-8221-509b97a4ed22 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a104bec7-1b86-4a0e-8221-509b97a4ed22');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-888aba1a-ca14-4e94-bb53-6e598dc05751\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-888aba1a-ca14-4e94-bb53-6e598dc05751')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-888aba1a-ca14-4e94-bb53-6e598dc05751 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of labels\n",
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gyzpmrNqWN7",
        "outputId": "29143164-161c-4139-cb9f-0ac2ee183980"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "METHODS        59353\n",
              "RESULTS        57953\n",
              "CONCLUSIONS    27168\n",
              "BACKGROUND     21727\n",
              "OBJECTIVE      13839\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The length of different lines\n",
        "train_df.total_lines.plot.hist(bins=15);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "zDXG0C9WqaJ8",
        "outputId": "72753bd2-fea1-41e2-bdbf-6e959da0d610"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the abstracts have total lines between `(5, 18)`."
      ],
      "metadata": {
        "id": "ufbjr7rorLLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get list of sentences"
      ],
      "metadata": {
        "id": "GhDjegsVqn7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert abstract text lines into lists\n",
        "# train_sentences = train_df[\"text\"].to_numpy()\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiwP-3NXqvYj",
        "outputId": "35257a26-81df-49a8-9af5-0ee14cd484ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make numeric labels (ML models require numeric labels)"
      ],
      "metadata": {
        "id": "6zsZyQGvriS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot encoding"
      ],
      "metadata": {
        "id": "XRj0gL6FuybK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoder\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False) # tf doesn't support tensorflow sparse matrix format so we set it False\n",
        "# ordinal_encoder = OrdinalEncoder(dtype='int32')\n",
        "\n",
        "one_hot_encoder.fit(train_df[[\"target\"]])\n",
        "train_labels_onehot = one_hot_encoder.transform(train_df[[\"target\"]])\n",
        "train_labels_onehot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biLXTiZhrDYw",
        "outputId": "17a5eb27-3c17-485b-a0e9-a1158da025b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_labels_onehot = one_hot_encoder.transform(val_df[[\"target\"]])\n",
        "test_labels_onehot = one_hot_encoder.transform(test_df[[\"target\"]])"
      ],
      "metadata": {
        "id": "sotTPgrpuZg2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ordinal encoding"
      ],
      "metadata": {
        "id": "5VQ2XpaBu1NI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "ordinal_encoder.fit(train_df[[\"target\"]])\n",
        "train_labels = ordinal_encoder.transform(train_df[[\"target\"]])\n",
        "train_labels[:10]\n",
        "\n",
        "# array([[3],\n",
        "#        [2],\n",
        "#        [2],\n",
        "#        [2],\n",
        "#        [2],\n",
        "#        [2],\n",
        "#        [4],\n",
        "#        [4],\n",
        "#        [4],\n",
        "#        [4]], dtype=int32)\n",
        "```"
      ],
      "metadata": {
        "id": "mfs8WHeQtPrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label encoding"
      ],
      "metadata": {
        "id": "_JgxtcBWu34s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import Label\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df.target)\n",
        "val_labels_encoded = label_encoder.transform(val_df.target)\n",
        "test_labels_encoded = label_encoder.transform(test_df.target)\n",
        "\n",
        "# check the training labels\n",
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spTxv58gt5FV",
        "outputId": "b65851ff-2474-4512-8715-fd0b471eaf24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, ..., 4, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names and number of classes from LabelEncoder instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1CxBHjuvSsj",
        "outputId": "6a388326-54ce-4128-e589-861b46aa26ab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,\n",
              " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Series of Experiments\n",
        "\n",
        "Now we're going to try out a series of experiments and see which one works.\n",
        "\n",
        "And the first model is the baseline mode. Here we use TF-IDF Multinomial Naive Bayes Classifier.\n",
        "\n",
        "1. Naive Bayes with TF-IDF encoder\n",
        "2. Conv1D with token embeddings\n",
        "3. TensorFlow Hub Pretrained Feature Extractor\n",
        "4. Conv1D wuth character embeddings\n",
        "5. Pretrained token embeddings + character embeddings\n",
        "6. Pretrained token embeddings + character embeddings + positional embeddings"
      ],
      "metadata": {
        "id": "-CxRGEQiwCTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 0 - baseline (Naive Bayes)"
      ],
      "metadata": {
        "id": "V9vWYYHpwFcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model_0 = Pipeline([\n",
        "    ('tf-idf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "    ])\n",
        "\n",
        "model_0.fit(train_sentences, train_labels_encoded)\n",
        "model_0.score(val_sentences, val_labels_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfGji8ckvov4",
        "outputId": "3b410432-3f02-40c7-8600-476148201c02"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7218323844829869"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwKsf_bbziHJ",
        "outputId": "918d7116-6bbf-4abd-9aae-b85753b65907"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 1, 3, ..., 4, 4, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the preds and true labels using our helper function -> calculate_results()\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "baseline_results = calculate_results(val_labels_encoded, baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5fzkStUyUOA",
        "outputId": "c52ba8ae-8380-4956-b059-a07bcd8c876b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 72.1832384482987,\n",
              " 'precision': 0.7186466952323352,\n",
              " 'recall': 0.7218323844829869,\n",
              " 'f1': 0.6989250353450294}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ **Remider**:\n",
        "\n",
        "If the `train_sentences` is `numpy` type, then error:\n",
        "```\n",
        "AttributeError: 'numpy.ndarray' object has no attribute 'lower'\n",
        "```\n",
        "After change to `list` type, it works.\n"
      ],
      "metadata": {
        "id": "eRcavSSHxxl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing text to numeric format for deep sequence model\n",
        "\n",
        "Create vectorization and embedding layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "VpA63LzF0TlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "u1u10_Pkwayt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long is each sentence on average\n",
        "# Need to define the output length in textorization\n",
        "sentence_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sentence_len = np.mean(sentence_lens)\n",
        "avg_sentence_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDoMObOn1cBU",
        "outputId": "97f1d48c-8323-4ab3-c616-0eed8eea2612"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.338269273494777"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the average length of sentence in the training data set is 26"
      ],
      "metadata": {
        "id": "V0ACIImC10j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The distribution\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sentence_lens, bins=5);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "1PKJRNUW1zEg",
        "outputId": "3419b7ff-9bad-4e98-a5c3-4c94cdd335fc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How long of a sentence length cover 95% of examples?\n",
        "out_seq_len = int(np.percentile(sentence_lens, 95))\n",
        "out_seq_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbfxVnKY2Egi",
        "outputId": "ee835787-6241-409e-b3e9-5bb39ce09d44"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum sequence length\n",
        "max(sentence_lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy08sDOd25x9",
        "outputId": "d422a262-ed4b-410b-cf68-52b1b1382000"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "296"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 💡 The steps here help us define the `output_sequence_length` so that it can cover the majority of our training samples."
      ],
      "metadata": {
        "id": "vFxnmmz52PBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create text vectorizer layer\n",
        "\n",
        "We want to make a layer which maps our texts from words to numbers.\n",
        "\n",
        "> \"I love TensorFlow\" -> [0, 1, 2]\n",
        "> ```\n",
        "> I = 1\n",
        "> love = 2\n",
        "> TensorFlow = 3\n",
        "> ```"
      ],
      "metadata": {
        "id": "T2wU-MYt39Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words in our dataset (taken from the paper)\n",
        "max_vocab = 68000\n",
        "\n",
        "# Create text vectorizer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab,\n",
        "                                    output_sequence_length=out_seq_len)\n"
      ],
      "metadata": {
        "id": "rxepfwxU2t3x"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt to text vectorizer to training sentences\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "tUdd1I_K5nhX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out text vectorizer on random sentences\n",
        "import random\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(f\"Text:\\n {random_sentence}\")\n",
        "print(f\"Length of text: {len(random_sentence.split())}\\n\")\n",
        "print(f\"Vectorized text:\\n {text_vectorizer([random_sentence])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyou7k2Y5zy_",
        "outputId": "ee0737d2-fc4d-4d29-ef8b-928e2e9d7ae2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            " all cattle were ear-tagged , treated with diminazene diaceturate ( da ) and those in regimens @-@ received monthly graded rap .\n",
            "Length of text: 22\n",
            "\n",
            "Vectorized text:\n",
            " [[   62  6309     9 59093   172     7 31701 39672  2977     3   125     5\n",
            "    952    80  1097  3013 10192     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words in our training vocabulary\n",
        "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
        "print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
        "print(f\"Most common words: {rct_20k_text_vocab[:5]}\")\n",
        "print(f\"Most common words: {rct_20k_text_vocab[-5:]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOjl-vhV6Wry",
        "outputId": "9f6c6f06-a4f3-4d95-81d2-6e9eb1c831df"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in vocab: 64841\n",
            "Most common words: ['', '[UNK]', 'the', 'and', 'of']\n",
            "Most common words: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get all the parameters of our text vectorizer\n",
        "text_vectorizer.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3DpRNtU694r",
        "outputId": "1f755226-894d-4065-a9db-fd62862b6f27"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'text_vectorization',\n",
              " 'trainable': True,\n",
              " 'dtype': 'string',\n",
              " 'batch_input_shape': (None,),\n",
              " 'max_tokens': 68000,\n",
              " 'standardize': 'lower_and_strip_punctuation',\n",
              " 'split': 'whitespace',\n",
              " 'ngrams': None,\n",
              " 'output_mode': 'int',\n",
              " 'output_sequence_length': 55,\n",
              " 'pad_to_max_tokens': False,\n",
              " 'sparse': False,\n",
              " 'ragged': False,\n",
              " 'vocabulary': None,\n",
              " 'idf_weights': None,\n",
              " 'encoding': 'utf-8',\n",
              " 'vocabulary_size': 64841}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create custom text embedding"
      ],
      "metadata": {
        "id": "WpWaHPlC7Wzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # all vocab in training dataset\n",
        "                               output_dim=128, # the larger output dim, the longer training time\n",
        "                               mask_zero=True, # the benefit is saving space\n",
        "                               name=\"token_embedding\")"
      ],
      "metadata": {
        "id": "s7fg_Ylp7Q_9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`mask_zero=True` means that if there are lost of zeroes in inputs, the embedding layer will mask it so that improve computing efficiency."
      ],
      "metadata": {
        "id": "ZOFe1NtNAahB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example embedding\n",
        "print(f\"Sentence before vectorization: \\n {random_sentence}\")\n",
        "vectorized_sentence = text_vectorizer([random_sentence])\n",
        "print(f\"After vectorization before embedding:\\n {vectorized_sentence}\")\n",
        "embedded_sentence = token_embed(vectorized_sentence)\n",
        "print(f\"Sentence after embedding:\\n {embedded_sentence}\")\n",
        "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmDeLP0k7itP",
        "outputId": "d0057229-d3f0-4896-c067-9f9632cda675"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence before vectorization: \n",
            " all cattle were ear-tagged , treated with diminazene diaceturate ( da ) and those in regimens @-@ received monthly graded rap .\n",
            "After vectorization before embedding:\n",
            " [[   62  6309     9 59093   172     7 31701 39672  2977     3   125     5\n",
            "    952    80  1097  3013 10192     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0]]\n",
            "Sentence after embedding:\n",
            " [[[ 0.00198573 -0.00765151 -0.00736092 ... -0.03147845 -0.02108963\n",
            "    0.04023762]\n",
            "  [-0.03043673 -0.03436925 -0.00029727 ... -0.00186272  0.04405091\n",
            "    0.00802635]\n",
            "  [-0.00734006 -0.01062737 -0.01148904 ...  0.02566249  0.01740395\n",
            "   -0.02945696]\n",
            "  ...\n",
            "  [-0.02083162  0.03269492 -0.00669986 ... -0.0279091  -0.01687013\n",
            "   -0.01941784]\n",
            "  [-0.02083162  0.03269492 -0.00669986 ... -0.0279091  -0.01687013\n",
            "   -0.01941784]\n",
            "  [-0.02083162  0.03269492 -0.00669986 ... -0.0279091  -0.01687013\n",
            "   -0.01941784]]]\n",
            "Embedded sentence shape: (1, 55, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each words in vectorized sentence (55 in total) is represented by a 128-dim vector."
      ],
      "metadata": {
        "id": "2TizeBkNBbPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating datasets (making sure our data loads as fast as possible)\n",
        "\n",
        "We're going to setup our data to run as fast as possible with the TensorFlow tf.data API, see resources:\n",
        "* https://www.tensorflow.org/guide/data_performance\n",
        "* https://www.tensorflow.org/guide/data"
      ],
      "metadata": {
        "id": "xP2VNaSXHVjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_onehot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_onehot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_onehot))"
      ],
      "metadata": {
        "id": "OBB7dUi6HxM0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn into prefetch datasets\n",
        "# Here we want the model to learn the order of inputs so we don't need to shuffle the datasets\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Qrk_UK21IsxM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 - Conv1D"
      ],
      "metadata": {
        "id": "rYgEACxmBaxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "# model_1 = tf.keras.Sequential([\n",
        "#     text_vectorizer(),\n",
        "#     token_embed(),\n",
        "#     layers.Conv1D(),\n",
        "#     layers.AveragePooling1D(),\n",
        "#     layers.Dense(5)\n",
        "# ])\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = token_embed(x)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding='same', activation='relu')(x)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x) # we are working on multiclasses classification - softmax\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "ydwIFR_cBT42"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8CsfasfCyRa",
        "outputId": "9a78cc6b-072e-4a62-e38b-6b893101b4fc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVe  (None, 55)                0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " token_embedding (Embedding  (None, 55, 128)           8299648   \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 55, 64)            41024     \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 64)                0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8340997 (31.82 MB)\n",
            "Trainable params: 8340997 (31.82 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we have 5627 batches (still large)\n",
        "len(train_dataset), len(train_sentences)/32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWjtzXSLKqQe",
        "outputId": "e1356225-e134-459a-a4a8-dc0a27d77b2c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5627, 5626.25)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_history = model_1.fit(train_dataset, epochs=5,\n",
        "                              steps_per_epoch=len(train_dataset),\n",
        "                              # steps_per_epoch=int(0.1*len(train_dataset)), # we train on 10% of training data to get results quicker so we can check what is going well faster.\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1*len(valid_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln-IWf-7KioQ",
        "outputId": "20d1dc8c-0e25-41ad-e75b-a27a9077881f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "5627/5627 [==============================] - 395s 70ms/step - loss: 0.5508 - accuracy: 0.7968 - val_loss: 0.4875 - val_accuracy: 0.8228\n",
            "Epoch 2/5\n",
            "5627/5627 [==============================] - 393s 70ms/step - loss: 0.3598 - accuracy: 0.8721 - val_loss: 0.5138 - val_accuracy: 0.8225\n",
            "Epoch 3/5\n",
            "5627/5627 [==============================] - 393s 70ms/step - loss: 0.2151 - accuracy: 0.9302 - val_loss: 0.6043 - val_accuracy: 0.8085\n",
            "Epoch 4/5\n",
            "5627/5627 [==============================] - 381s 68ms/step - loss: 0.1112 - accuracy: 0.9675 - val_loss: 0.7394 - val_accuracy: 0.8015\n",
            "Epoch 5/5\n",
            "5627/5627 [==============================] - 410s 73ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.8734 - val_accuracy: 0.7962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training process for 5 epoch:\n",
        "```\n",
        "Epoch 1/5\n",
        "5627/5627 [==============================] - 395s 70ms/step - loss: 0.5508 - accuracy: 0.7968 - val_loss: 0.4875 - val_accuracy: 0.8228\n",
        "Epoch 2/5\n",
        "5627/5627 [==============================] - 393s 70ms/step - loss: 0.3598 - accuracy: 0.8721 - val_loss: 0.5138 - val_accuracy: 0.8225\n",
        "Epoch 3/5\n",
        "5627/5627 [==============================] - 393s 70ms/step - loss: 0.2151 - accuracy: 0.9302 - val_loss: 0.6043 - val_accuracy: 0.8085\n",
        "Epoch 4/5\n",
        "5627/5627 [==============================] - 381s 68ms/step - loss: 0.1112 - accuracy: 0.9675 - val_loss: 0.7394 - val_accuracy: 0.8015\n",
        "Epoch 5/5\n",
        "5627/5627 [==============================] - 410s 73ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.8734 - val_accuracy: 0.7962\n",
        "```"
      ],
      "metadata": {
        "id": "4G--6RKcwxKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.evaluate(valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeZV6Zq4Lc1k",
        "outputId": "769d08c6-e1a2-4183-d103-e18cfe8e26f1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 5s 5ms/step - loss: 0.8991 - accuracy: 0.7947\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8990655541419983, 0.7946511507034302]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred_probs = model_1.predict(valid_dataset)\n",
        "model_1_pred_probs, model_1_pred_probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CiTxgOVLoh4",
        "outputId": "684a839f-0cb8-43f4-8785-96870762e9e2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 4s 5ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[8.0848920e-01, 1.4383736e-01, 2.3644911e-02, 2.0548189e-02,\n",
              "         3.4802409e-03],\n",
              "        [6.0596240e-01, 1.7793642e-02, 9.7530366e-09, 3.7624362e-01,\n",
              "         3.6271382e-07],\n",
              "        [6.5723856e-05, 5.1268111e-05, 3.9899983e-10, 9.9988288e-01,\n",
              "         6.6556121e-09],\n",
              "        ...,\n",
              "        [1.5413663e-09, 2.0147159e-09, 1.0775945e-03, 3.8620474e-08,\n",
              "         9.9892235e-01],\n",
              "        [2.5245862e-04, 9.9112976e-01, 1.5007122e-05, 1.6297885e-06,\n",
              "         8.6011039e-03],\n",
              "        [3.7682989e-07, 9.9999952e-01, 3.1085207e-08, 2.0155051e-08,\n",
              "         1.5439485e-07]], dtype=float32),\n",
              " (30212, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert pred_probs to classes\n",
        "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
        "model_1_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YruyNvnsBGm",
        "outputId": "acfc2a6b-cecc-4e77-f7f8-418ac1b159c1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_1 results\n",
        "model_1_results = calculate_results(val_labels_encoded, model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUrIF5ylsTV8",
        "outputId": "eaca48ad-ae22-4ed8-9b6b-2ae683557f02"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.46511320005297,\n",
              " 'precision': 0.7926879260877197,\n",
              " 'recall': 0.7946511320005296,\n",
              " 'f1': 0.7927004130055443}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsW9ujvTxAqy",
        "outputId": "c550dbb5-dd37-41e0-c6ec-5389c1921ec3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 72.1832384482987,\n",
              " 'precision': 0.7186466952323352,\n",
              " 'recall': 0.7218323844829869,\n",
              " 'f1': 0.6989250353450294}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎉 Great! We beat the baseline using only one convolutional layer."
      ],
      "metadata": {
        "id": "F7tx9fgWxC1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 - Feature extraction with pretrained token embeddings\n",
        "\n",
        "In the original paper, the authors used [GloVe embeddings](https://nlp.stanford.edu/projects/glove/), we could use it for our `model_2`. (extension)\n",
        "\n",
        "However, we are going to use more recent version of pretrained word embedding -- [universal sentence encoder(USE)](https://tfhub.dev/google/universal-sentence-encoder/4).\n",
        "\n",
        "Or, you can check more pretrained models on [Huggingface](https://huggingface.co/models).\n",
        "\n",
        "When you're using transfer learning, we would want to find the latest version of some pretrained feature extractors/embeddings.\n",
        "\n",
        "----\n",
        "We'll keep the pretrained embeddings frozen (by setting `trainable=False`) and add a trainable couple of layers on the top to tailor the model outputs to our own data."
      ],
      "metadata": {
        "id": "GnwTLehJuiMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained Tensorflow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                 trainable=False,\n",
        "                                 name=\"universal_sentence_encoder\") # output is 512 dimensional vector"
      ],
      "metadata": {
        "id": "fnySbThavuy_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the pretrained embedding on a random sentence\n",
        "# We don't need to vectorize sentences using USE embedding\n",
        "random_train_sentence = random.choice(train_sentences)\n",
        "print(f\"Random sentence:\\n {random_train_sentence}\")\n",
        "use_embedded_sentence = embedding_layer([random_train_sentence])\n",
        "print(f\"Sentence after embeded:\\n {use_embedded_sentence[0][:30]}\") # first 30\n",
        "print(f\"Length of sentence embedding: {len(use_embedded_sentence[0])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ4WN4oMxTgF",
        "outputId": "61e8c10d-a3f4-4936-80bb-19cb1c849ed2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random sentence:\n",
            " the response rate was @ % with nivolumab versus @ % with docetaxel ( p = @ ) .\n",
            "Sentence after embeded:\n",
            " [ 0.01381422 -0.07788073  0.00224727 -0.0500415  -0.02583176  0.03453026\n",
            "  0.03104751 -0.06666333  0.05171972  0.03999669  0.00215903  0.06156719\n",
            "  0.00516985 -0.00302238  0.01961534 -0.00388749  0.06165432 -0.00642532\n",
            "  0.04615     0.01207109  0.08077908 -0.03281293  0.01646825  0.02228799\n",
            " -0.03198744  0.01979592 -0.06054161  0.0290057  -0.08539628  0.05554584]\n",
            "Length of sentence embedding: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model 2\n",
        "# wrong way: inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "inputs = layers.Input(shape=[], dtype=tf.string)\n",
        "pretrained_embedding = embedding_layer(inputs)\n",
        "x = layers.Dense(128, activation='relu')(pretrained_embedding)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "nAKpJZ4ZyDEU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "❓Question: Why not use convolution layer?"
      ],
      "metadata": {
        "id": "-ZBLLtKl2nRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzQiw1KX1p_9",
        "outputId": "9bd5ac95-18d1-433a-9991-ccc70e767138"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None,)]                 0         \n",
            "                                                                 \n",
            " universal_sentence_encoder  (None, 512)               256797824 \n",
            "  (KerasLayer)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 256864133 (979.86 MB)\n",
            "Trainable params: 66309 (259.02 KB)\n",
            "Non-trainable params: 256797824 (979.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_history = model_2.fit(train_dataset,\n",
        "                              epochs=3,\n",
        "                              steps_per_epoch=int(0.1*len(train_dataset)),\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1*len(valid_dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M86J29_1smM",
        "outputId": "7e84ca8c-52fb-458c-b9da-0ae1a98b2e49"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "562/562 [==============================] - 10s 11ms/step - loss: 0.9191 - accuracy: 0.6477 - val_loss: 0.7978 - val_accuracy: 0.6888\n",
            "Epoch 2/3\n",
            "562/562 [==============================] - 9s 16ms/step - loss: 0.7684 - accuracy: 0.7018 - val_loss: 0.7542 - val_accuracy: 0.7061\n",
            "Epoch 3/3\n",
            "562/562 [==============================] - 7s 13ms/step - loss: 0.7508 - accuracy: 0.7124 - val_loss: 0.7365 - val_accuracy: 0.7171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model_2` is training faster than `model_1`. That's because `model_2` doesn't have to train the embedding layer since we set `trainable=False`, while in `model_1`, we have to train our own embedding layer."
      ],
      "metadata": {
        "id": "cY2mK2tu29Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.evaluate(valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GJrIfx12xS-",
        "outputId": "9741b8f1-4f32-4db5-fe1d-5774778fe25f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 15s 16ms/step - loss: 0.7382 - accuracy: 0.7156\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7382416725158691, 0.7156096696853638]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_preds = tf.argmax(model_2.predict(valid_dataset), axis=1)\n",
        "model_2_preds[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGv-RZbZ3RZG",
        "outputId": "fd417508-2af3-4aaa-ef4a-91d34e5b5ce0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 8s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 3, 2, 4, 2, 2, 2, 4, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results from TF hub pretrained embedding\n",
        "model_2_results = calculate_results(val_labels_encoded, model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLbPpTW-3gNJ",
        "outputId": "9b81aecf-0740-43a6-e769-7909c07c1cd3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 71.5609691513306,\n",
              " 'precision': 0.7157621058131338,\n",
              " 'recall': 0.715609691513306,\n",
              " 'f1': 0.7128240371038443}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNGkf8ie4AzQ",
        "outputId": "5a601519-863d-4462-8c7b-51ff41f756f9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 72.1832384482987,\n",
              " 'precision': 0.7186466952323352,\n",
              " 'recall': 0.7218323844829869,\n",
              " 'f1': 0.6989250353450294}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, it seems like `model_2` does not outperform the baseline. It's okay, we will keep doing experiments."
      ],
      "metadata": {
        "id": "ZQcOKbO34CpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3 - Conv1D with Character embeddings."
      ],
      "metadata": {
        "id": "f_bmUi9b4O6e"
      }
    }
  ]
}